{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COMP615 - Assignment Two Notebook (Commented)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports\n",
    "This cell imports all the necessary libraries for the project.\n",
    "- `pandas` for data manipulation and reading CSV files.\n",
    "- `numpy` for numerical operations.\n",
    "- `matplotlib` and `seaborn` for data visualization.\n",
    "- `sklearn` for machine learning models, preprocessing, and evaluation metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data analysis and visualization libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Import tools for model selection and preprocessing from scikit-learn\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "\n",
    "# Import machine learning models\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.naive_bayes import GaussianNB \n",
    "\n",
    "# Import tools for evaluating model performance\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "import warnings\n",
    "\n",
    "# Configure warnings to be ignored for a cleaner output\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading\n",
    "Here, we load the training and testing datasets from the CSV files into pandas DataFrames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the training data into a DataFrame named 'df'\n",
    "df = pd.read_csv('training.csv')\n",
    "# Read the testing data into a DataFrame named 'dftest'\n",
    "dftest = pd.read_csv('testing.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part A: K-Nearest Neighbors (KNN) and Na√Øve Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A2. Perform Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Overview and Integrity Check\n",
    "This section provides a summary of the datasets and checks for any data quality issues like missing or duplicate values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display a concise summary of the training DataFrame, including data types and non-null values\n",
    "print(\"--- Training Set Information ---\")\n",
    "df.info()\n",
    "\n",
    "# Display a concise summary of the testing DataFrame\n",
    "print(\"\\n\\n--- Testing Set Information ---\")\n",
    "dftest.info()\n",
    "\n",
    "# Check for and report the total number of missing (null) and duplicate rows\n",
    "print('\\n\\n--- Null and Duplicate Value Check ---')\n",
    "print('\\nTraining Set:')\n",
    "print(f\"Total null values: {df.isnull().sum().sum()}\")\n",
    "print(f\"Total duplicate values: {df.duplicated().sum()}\")\n",
    "\n",
    "print('\\nTesting Set:')\n",
    "print(f\"Total null values: {dftest.isnull().sum().sum()}\")\n",
    "print(f\"Total duplicate values: {dftest.duplicated().sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class Distribution\n",
    "This plot shows the number of samples for each class in the target variable to see if the dataset is balanced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the figure size for the plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "# Create a bar plot showing the count of each class\n",
    "sns.countplot(x='class', data=df, order=df['class'].value_counts().index)\n",
    "# Set the title and labels for the plot\n",
    "plt.title('Class Distribution')\n",
    "plt.xlabel('Class')\n",
    "plt.ylabel('Frequency (Count)')\n",
    "# Rotate the x-axis labels for better readability\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "# Adjust layout to prevent labels from overlapping\n",
    "plt.tight_layout()\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Visualization: Feature Characteristics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Histogram of Feature Variances\n",
    "This histogram shows the distribution of variances for all numerical features. Features with very low variance might not be very informative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new DataFrame without the 'class' column to analyze numerical features\n",
    "numeric_df = df.drop('class', axis=1)\n",
    "# Calculate the variance for each feature\n",
    "feature_variances = numeric_df.var()\n",
    "\n",
    "# Create the histogram plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(feature_variances, bins=30, edgecolor='black')\n",
    "plt.title('Histogram of Feature Variances')\n",
    "plt.xlabel('Variance')\n",
    "plt.ylabel('Number of Features (Frequency)')\n",
    "plt.grid(axis='y', alpha=0.75)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Correlation Heatmap\n",
    "A heatmap visualizes the correlation between features. High correlation (values near 1 or -1) indicates that features are related, which is important to know for certain models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the correlation matrix for the numerical features\n",
    "correlation_matrix = numeric_df.corr()\n",
    "\n",
    "# Create the heatmap plot\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(correlation_matrix,\n",
    "            annot=False,      # Do not display the correlation values on the map\n",
    "            cmap='coolwarm',  # Use a color scheme where red is positive and blue is negative correlation\n",
    "            vmin=-1,          # Set the minimum value for the color scale\n",
    "            vmax=1,           # Set the maximum value for the color scale\n",
    "            center=0)         # Center the color scale at zero\n",
    "plt.title('Correlation Heatmap of Features', fontsize=15)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Boxplots for Selected Features by Class\n",
    "Boxplots help visualize the distribution of a feature's values for each class. This can show if a feature is good at separating different classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the target column and select a few features to visualize\n",
    "target_column_name = 'class'\n",
    "selected_features_for_boxplot = ['NDVI', 'Mean_NIR', 'GLCM2', 'Bright', 'Mean_R']\n",
    "\n",
    "# Create a grid of boxplots\n",
    "plt.figure(figsize=(18, 12))\n",
    "for i, feature_name in enumerate(selected_features_for_boxplot):\n",
    "   # Create a subplot for each feature\n",
    "   plt.subplot(2, 3, i + 1)\n",
    "   sns.boxplot(x=target_column_name, y=feature_name, data=df)\n",
    "   plt.title(f'Boxplot of {feature_name}\\nby {target_column_name}')\n",
    "   plt.xlabel(target_column_name)\n",
    "   plt.ylabel(feature_name)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A3. Feature Selection and Analysis\n",
    "This section uses a statistical test (ANOVA F-test) to automatically select the 5 most influential features for predicting the target class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features (X) and the target variable (y)\n",
    "X = df.drop('class', axis=1)\n",
    "y = df['class']\n",
    "\n",
    "# Encode the categorical target variable 'y' into numerical labels\n",
    "le = LabelEncoder()\n",
    "y_encoded = le.fit_transform(y)\n",
    "\n",
    "# Use SelectKBest to find the top 5 features using the ANOVA F-test (f_classif)\n",
    "selector = SelectKBest(score_func=f_classif, k=5)\n",
    "selector.fit(X, y)\n",
    "\n",
    "# Get the names of the selected features\n",
    "top_features_names = X.columns[selector.get_support()].tolist()\n",
    "\n",
    "# Print the names of the top 5 features\n",
    "print(\"Top 5 features by F score:\")\n",
    "for name in top_features_names:\n",
    "    print(f\"- {name}\")\n",
    "\n",
    "# Create a temporary DataFrame for plotting the selected features\n",
    "df_plot_selected = X[top_features_names].copy()\n",
    "df_plot_selected['class'] = y.reset_index(drop=True)\n",
    "class_order_for_plot = df['class'].value_counts().index\n",
    "\n",
    "# Create a boxplot for each of the top 5 features to see how they separate the classes\n",
    "for feature_name in top_features_names:\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    sns.boxplot(x='class', y=feature_name, data=df_plot_selected, order=class_order_for_plot)\n",
    "    plt.title(f'Distribution of  {feature_name} by Class')\n",
    "    plt.ylabel(f'{feature_name}')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing for Models\n",
    "Before training the models, we need to prepare the data. This involves separating features from the target and scaling the features. Scaling is important for models like KNN and MLP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features (X) from the target (y) for both training and testing sets\n",
    "X_train = df.drop('class', axis=1)\n",
    "y_train = df['class']\n",
    "\n",
    "X_test = dftest.drop('class', axis= 1)\n",
    "y_test = dftest['class']\n",
    "\n",
    "# Initialize the StandardScaler\n",
    "scaler = StandardScaler()\n",
    "# Fit the scaler on the training data and transform it\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "# Transform the test data using the scaler that was fitted on the training data\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(f\"Training data scaled. Shape: {X_train_scaled.shape}\")\n",
    "print(f\"Test data scaled. Shape: {X_test_scaled.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A6. K-Nearest Neighbors (KNN) Model Building and Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding the Optimal 'k' using Cross-Validation\n",
    "We test different values for 'k' (the number of neighbors) to find the one that gives the best performance on average, using cross-validation to prevent overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the range of 'k' values to test (from 1 to 20)\n",
    "k_range = range(1, 21)\n",
    "# Create an empty list to store the cross-validation scores\n",
    "cv_scores = []\n",
    "\n",
    "print(\"--- Finding Best K using 5-Fold Cross-Validation ---\")\n",
    "# Loop through each k value in the range\n",
    "for k_val in k_range:\n",
    "    # Initialize the KNN model with the current k value\n",
    "    knn = KNeighborsClassifier(n_neighbors=k_val)\n",
    "    # Perform 5-fold cross-validation and get the accuracy scores\n",
    "    scores = cross_val_score(knn, X_train_scaled, y_train, cv=5, scoring='accuracy')\n",
    "    # Append the mean of the scores to our list\n",
    "    cv_scores.append(scores.mean())\n",
    "    print(f\"k={k_val}, CV Mean Accuracy: {scores.mean():.4f}\")\n",
    "\n",
    "# Find the k value that resulted in the highest cross-validation accuracy\n",
    "best_k_cv = k_range[np.argmax(cv_scores)]\n",
    "print(f\"\\nBest k based on cross-validation: {best_k_cv} with CV accuracy: {max(cv_scores):.4f}\")\n",
    "\n",
    "# Plot the accuracy scores for each k value to visualize the result\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(k_range, cv_scores, marker='o', linestyle='dashed')\n",
    "plt.title('KNN Performance for different k values (Cross-Validation)')\n",
    "plt.xlabel('Number of Neighbors (k)')\n",
    "plt.ylabel('Mean Cross-Validated Accuracy')\n",
    "plt.xticks(k_range)\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final KNN Model Evaluation\n",
    "Using the best 'k' we found, we train the final KNN model on the entire training set and evaluate its performance on the separate test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set k to the best value found during cross-validation\n",
    "k = best_k_cv\n",
    "# Initialize the final KNN model\n",
    "knn_model = KNeighborsClassifier(n_neighbors=k)\n",
    "\n",
    "# Train the model on the scaled training data\n",
    "knn_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Make predictions on the scaled test data\n",
    "y_pred_knn = knn_model.predict(X_test_scaled)\n",
    "\n",
    "# Calculate the accuracy of the model on the test data\n",
    "accuracy = accuracy_score(y_test, y_pred_knn)\n",
    "print(f\"KNN Model Accuracy (k={k}): {accuracy:.4f}\")\n",
    "\n",
    "# Print a detailed classification report (precision, recall, f1-score)\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred_knn))\n",
    "\n",
    "# Get the unique class labels for the confusion matrix\n",
    "class_labels = np.unique(np.concatenate((y_test, y_pred_knn)))\n",
    "\n",
    "# Generate the confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred_knn, labels=class_labels)\n",
    "\n",
    "# Create a DataFrame from the confusion matrix for better visualization\n",
    "cm_df = pd.DataFrame(cm,\n",
    "                     index = class_labels,\n",
    "                     columns = class_labels)\n",
    "\n",
    "# Plot the confusion matrix as a heatmap\n",
    "plt.figure(figsize=(10, 7))\n",
    "sns.heatmap(cm_df,\n",
    "            annot=True,      # Show the numbers in each cell\n",
    "            fmt='d',         # Format numbers as integers\n",
    "            cmap='Blues',    # Use the 'Blues' color scheme\n",
    "            linewidths=.5,\n",
    "            linecolor='gray',\n",
    "            cbar=True)\n",
    "\n",
    "plt.title(f'Confusion Matrix for KNN (k={best_k_cv})', fontsize=15)\n",
    "plt.ylabel('True Label', fontsize=12)\n",
    "plt.xlabel('Predicted Label', fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part B: Exploring Artificial Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B2 & B3. Baseline Model (Single Hidden Layer) and Loss Tracking\n",
    "We use `GridSearchCV` to find the best hyperparameters (number of neurons and iterations) for a single-layer MLP. Then, we train the best model and plot its learning curve to see how training loss and validation accuracy change over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- Part B(b): Finding Best Parameters for Single-Layer MLP ---\")\n",
    "\n",
    "# Define the grid of hyperparameters to search\n",
    "param_grid = {\n",
    "    'hidden_layer_sizes': [(k,) for k in range(5, 26, 5)], # Test architectures with one layer of 5, 10, 15, 20, or 25 neurons\n",
    "    'max_iter': [50, 100, 150, 200, 250]                    # Test different numbers of training iterations\n",
    "}\n",
    "\n",
    "# Set up GridSearchCV to automate the hyperparameter search with 10-fold cross-validation\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=MLPClassifier(random_state=42),\n",
    "    param_grid=param_grid,\n",
    "    cv=10,\n",
    "    scoring='accuracy',\n",
    "    n_jobs=-1,  # Use all available CPU cores to speed up the search\n",
    "    verbose=1   # Print progress updates\n",
    ")\n",
    "\n",
    "print(\"--- Starting GridSearchCV to find best hyperparameters ---\")\n",
    "# Run the search on the scaled training data\n",
    "grid_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Report the best parameters found by the search\n",
    "print(\"\\n--- Grid Search Results ---\")\n",
    "best_params_single_layer = grid_search.best_params_\n",
    "baseline_accuracy = grid_search.best_score_\n",
    "print(f\"Best parameters found: {best_params_single_layer}\")\n",
    "print(f\"Highest 10-fold CV Accuracy: {baseline_accuracy:.4f}\")\n",
    "\n",
    "print(\"\\n--- Part B(c): Creating and training the optimal model for plotting ---\")\n",
    "\n",
    "# Encode the target variable to integer labels, which MLPClassifier requires\n",
    "le = LabelEncoder()\n",
    "y_train_encoded = le.fit_transform(y_train)\n",
    "y_test_encoded = le.transform(y_test)\n",
    "\n",
    "# Create a new MLP model with the best parameters and enable early stopping\n",
    "optimal_mlp = MLPClassifier(\n",
    "    **best_params_single_layer, # Use the best parameters found (e.g., {'hidden_layer_sizes': (15,), 'max_iter': 200})\n",
    "    random_state=42,\n",
    "    early_stopping=True,        # Enable early stopping to prevent overfitting\n",
    "    validation_fraction=0.1,    # Use 10% of training data for validation\n",
    "    n_iter_no_change=15         # Stop if validation score doesn't improve for 15 consecutive iterations\n",
    ")\n",
    "\n",
    "# Train the final, optimal model\n",
    "optimal_mlp.fit(X_train_scaled, y_train_encoded)\n",
    "\n",
    "# Evaluate and print the final model's performance on the test set\n",
    "final_accuracy = optimal_mlp.score(X_test_scaled, y_test_encoded)\n",
    "print(f\"Final test accuracy of the optimal model: {final_accuracy:.4f}\")\n",
    "print(f\"Number of iterations run (due to early stopping): {optimal_mlp.n_iter_}\")\n",
    "\n",
    "# Create a plot to visualize the training process\n",
    "plt.figure(figsize=(12, 7))\n",
    "\n",
    "# Plot the training loss on the primary y-axis\n",
    "plt.plot(optimal_mlp.loss_curve_, label='Training Loss', color='blue')\n",
    "k_mlp = optimal_mlp.hidden_layer_sizes[0]\n",
    "plt.title(f'Optimal MLP (k={k_mlp}) - Loss & Validation Accuracy vs. Iterations', fontsize=14)\n",
    "plt.xlabel('Iteration (Epoch)', fontsize=12)\n",
    "plt.ylabel('Training Loss', color='blue', fontsize=12)\n",
    "plt.legend(loc='upper left')\n",
    "plt.grid(True)\n",
    "\n",
    "# Create a second y-axis to plot the validation accuracy on the same graph\n",
    "ax2 = plt.gca().twinx()\n",
    "ax2.plot(optimal_mlp.validation_scores_, label='Validation Accuracy', color='green', linestyle='--')\n",
    "ax2.set_ylabel('Validation Accuracy', color='green', fontsize=12)\n",
    "ax2.legend(loc='upper right')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B4. Experimenting with Two Hidden Layers\n",
    "This section tests if a deeper network (two hidden layers) can achieve better performance than the single-layer baseline. We split the total number of neurons from the best single-layer model across two layers in all possible ways."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the optimal number of neurons and iterations from the single-layer experiment\n",
    "best_k_from_single = best_params_single_layer['hidden_layer_sizes'][0]\n",
    "iterations_to_use = best_params_single_layer['max_iter']\n",
    "\n",
    "print(f\"\\n--- Part B(d): Experimenting with Two Hidden Layers ---\")\n",
    "print(f\"Total neurons to distribute: {best_k_from_single}\")\n",
    "print(f\"Iterations for each model: {iterations_to_use}\\n\")\n",
    "\n",
    "# Create a list to store the results of each two-layer configuration\n",
    "results_table_data = []\n",
    "\n",
    "# Loop through all possible ways to split the neurons between two layers\n",
    "for n2_neurons in range(1, best_k_from_single):\n",
    "    # Calculate the number of neurons for the first layer\n",
    "    n1_neurons = best_k_from_single - n2_neurons\n",
    "    # Define the two-layer architecture\n",
    "    layer_config = (n1_neurons, n2_neurons)\n",
    "\n",
    "    # Initialize the MLP model with the two-layer configuration\n",
    "    mlp_model = MLPClassifier(\n",
    "        hidden_layer_sizes=layer_config,\n",
    "        max_iter=iterations_to_use,\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    # Evaluate this configuration using 10-fold cross-validation\n",
    "    cv_scores = cross_val_score(mlp_model, X_train_scaled, y_train, cv=10, scoring='accuracy', n_jobs=1)\n",
    "    # Calculate the average accuracy\n",
    "    avg_accuracy = np.mean(cv_scores)\n",
    "\n",
    "    # Store the results\n",
    "    results_table_data.append({\n",
    "        'config': f\"({n1_neurons}, {n2_neurons})\",\n",
    "        'accuracy': avg_accuracy\n",
    "    })\n",
    "    print(f\"Configuration: {layer_config}, Avg 10-fold CV Accuracy: {avg_accuracy:.4f}\")\n",
    "\n",
    "\n",
    "# Print a summary table of the results\n",
    "print(\"\\n--- Summary Table for Two-Layer Configurations ---\")\n",
    "print(\"---------------------------------------------\")\n",
    "print(\"| Neuron Combination | Avg CV Accuracy    |\")\n",
    "print(\"|--------------------|--------------------|\")\n",
    "\n",
    "best_two_layer_config = \"\"\n",
    "best_two_layer_accuracy = 0.0\n",
    "\n",
    "for result in results_table_data:\n",
    "    print(f\"| {result['config']:<18} | {result['accuracy']:.4f}           |\")\n",
    "    # Keep track of the best-performing two-layer configuration\n",
    "    if result['accuracy'] > best_two_layer_accuracy:\n",
    "        best_two_layer_accuracy = result['accuracy']\n",
    "        best_two_layer_config = result['config']\n",
    "\n",
    "print(\"---------------------------------------------\")\n",
    "\n",
    "# Compare the best two-layer model with the best single-layer model\n",
    "print(\"\\n--- Final Comparison ---\")\n",
    "print(f\"Best single-layer baseline accuracy (from Part B(b)): {baseline_accuracy:.4f}\")\n",
    "print(f\"Best two-layer configuration found: {best_two_layer_config} with an accuracy of {best_two_layer_accuracy:.4f}\")\n",
    "\n",
    "if best_two_layer_accuracy > baseline_accuracy:\n",
    "    print(\"Conclusion: Adding a second hidden layer provided an improvement.\")\n",
    "else:\n",
    "    print(\"Conclusion: The single-layer architecture remains the best performing model.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}